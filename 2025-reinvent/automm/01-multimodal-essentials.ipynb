{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGluon MultiModal (AutoMM) - Essentials Workshop\n",
    "\n",
    "Welcome to the AutoGluon MultiModal essentials workshop! In this notebook, we'll explore how to use AutoGluon's `MultiModalPredictor` to solve problems involving multiple data modalities such as text, images, and tabular data.\n",
    "\n",
    "`MultiModalPredictor` is a powerful tool that can automatically build state-of-the-art deep learning models for inputs including images, text, and tabular data. It can predict the values of one column based on other features, handling the complexities of multiple data types seamlessly.\n",
    "\n",
    "## What You Will Learn\n",
    "\n",
    "* How to prepare multimodal data for AutoGluon\n",
    "* Training models with MultiModalPredictor\n",
    "* Making predictions on new data\n",
    "* Evaluating model performance\n",
    "* Handling different types of data modalities\n",
    "* Best practices for multimodal machine learning\n",
    "\n",
    "Let's get started by installing AutoGluon and importing the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q uv\n",
    "# Install AutoGluon and visualization dependencies\n",
    "!uv pip install -q sentencepiece==0.2.0\n",
    "!uv pip install autogluon==1.4\n",
    "!pip install -q matplotlib seaborn scikit-learn pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to MultiModal Learning\n",
    "\n",
    "Multimodal learning involves training models that can process and learn from multiple types of data modalities. In real-world applications, we often have access to data from various sources and formats:\n",
    "\n",
    "- **Text**: Product descriptions, reviews, articles\n",
    "- **Images**: Photos, diagrams, visualizations\n",
    "- **Documents**: PDFs, Word documents, HTML pages\n",
    "- **Tabular data**: Structured information in rows and columns\n",
    "\n",
    "Traditional machine learning approaches often require separate models for each data type or extensive feature engineering to combine them. AutoGluon's `MultiModalPredictor` simplifies this process by automatically handling the complexities of multimodal data and creating models that can leverage all available information sources.\n",
    "\n",
    "### Benefits of MultiModal Learning\n",
    "\n",
    "- **Complementary information**: Different modalities can provide complementary perspectives on the same problem\n",
    "- **Enhanced predictive power**: Combining modalities often leads to better performance than using a single modality\n",
    "- **Robustness**: The model can still make predictions if one modality is missing or noisy\n",
    "- **Real-world applicability**: Most real-world problems involve multiple data types\n",
    "\n",
    "AutoGluon's `MultiModalPredictor` is designed to make this powerful approach accessible to everyone, without requiring deep expertise in multimodal deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Example Dataset: PetFinder\n",
    "\n",
    "For this tutorial, we'll use a simplified and subsampled version of the [PetFinder dataset](https://www.kaggle.com/c/petfinder-adoption-prediction). The goal is to predict pet adoption rates based on their adoption profiles, which include images, text descriptions, and tabular features.\n",
    "\n",
    "In this simplified version, the adoption speed is grouped into two categories: 0 (slow) and 1 (fast).\n",
    "\n",
    "Let's download and prepare the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.core.utils.loaders import load_zip\n",
    "\n",
    "download_dir = './ag_multimodal_tutorial'\n",
    "zip_file = 'https://automl-mm-bench.s3.amazonaws.com/petfinder_for_tutorial.zip'\n",
    "\n",
    "# Download and unzip the dataset\n",
    "load_zip.unzip(zip_file, unzip_dir=download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the dataset into pandas DataFrames and examine it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = f'{download_dir}/petfinder_for_tutorial'\n",
    "\n",
    "train_data = pd.read_csv(f'{dataset_path}/train.csv', index_col=0)\n",
    "test_data = pd.read_csv(f'{dataset_path}/test.csv', index_col=0)\n",
    "\n",
    "label_col = 'AdoptionSpeed'\n",
    "\n",
    "# Display the first few rows of the training data\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Visualization for Multimodal Learning\n",
    "\n",
    "Let's explore the different data modalities in our dataset:\n",
    "\n",
    "1. **Tabular features**: Age, Breed, Color, etc.\n",
    "2. **Text**: Description of the pet\n",
    "3. **Images**: Photos of the pets\n",
    "\n",
    "For AutoGluon's `MultiModalPredictor`, image columns should contain strings whose values are paths to image files. Some records in our data have multiple images associated with them, but for simplicity, we'll use only the first image for each pet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_col = 'Images'\n",
    "\n",
    "# Take only the first image for each pet\n",
    "train_data[image_col] = train_data[image_col].apply(lambda ele: ele.split(';')[0])\n",
    "test_data[image_col] = test_data[image_col].apply(lambda ele: ele.split(';')[0])\n",
    "\n",
    "# Helper function to convert relative paths to absolute paths\n",
    "def path_expander(path, base_folder):\n",
    "    path_l = path.split(';')\n",
    "    return ';'.join([os.path.abspath(os.path.join(base_folder, path)) for path in path_l])\n",
    "\n",
    "# Convert relative paths to absolute paths\n",
    "train_data[image_col] = train_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\n",
    "test_data[image_col] = test_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring and Visualizing the Data\n",
    "\n",
    "Let's examine the different modalities in our dataset through visualizations to better understand what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a more comprehensive visualization of the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display, HTML\n",
    "from matplotlib import gridspec\n",
    "import random\n",
    "\n",
    "# Randomly sample indices for visualization\n",
    "sample_indices = random.sample(range(len(train_data)), 5)\n",
    "samples = train_data.iloc[sample_indices]\n",
    "\n",
    "# Create a figure for each sample instead of using gridspec\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    sample = train_data.iloc[idx]\n",
    "    \n",
    "    # Create a new figure for each sample\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Create a layout with subplots\n",
    "    plt.subplot(2, 2, 1)  # Top left for image\n",
    "    try:\n",
    "        img_path = sample[image_col]\n",
    "        img = plt.imread(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Pet #{idx} Image\", fontsize=12)\n",
    "        plt.axis('off')\n",
    "    except Exception as e:\n",
    "        plt.text(0.5, 0.5, f\"Error loading image: {e}\", ha='center', va='center')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    # Add pet info in top right\n",
    "    plt.subplot(2, 2, 2)\n",
    "    pet_type = \"Dog\" if sample['Type'] == 1 else \"Cat\"\n",
    "    adoption_speed = \"Fast\" if sample[label_col] == 1 else \"Slow\"\n",
    "    \n",
    "    pet_info = f\"Pet #{idx}: {pet_type}\\n\"\n",
    "    pet_info += f\"Age: {sample['Age']} months\\n\"\n",
    "    pet_info += f\"Gender: {'Male' if sample['Gender'] == 1 else 'Female'}\\n\"\n",
    "    pet_info += f\"Color: {sample['Color1']}\\n\"\n",
    "    pet_info += f\"Maturity Size: {sample['MaturitySize']}\\n\"\n",
    "    pet_info += f\"Adoption Speed: {adoption_speed}\\n\"\n",
    "    \n",
    "    plt.text(0.05, 0.5, pet_info, fontsize=12, va='center')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Pet Information\", fontsize=12)\n",
    "    \n",
    "    # Add text description in bottom part (spans full width)\n",
    "    plt.subplot(2, 1, 2)  # Bottom half\n",
    "    desc = sample['Description']\n",
    "    short_desc = desc[:400] + '...' if len(desc) > 400 else desc\n",
    "    plt.text(0.05, 0.9, \"Description:\", fontsize=12, fontweight='bold', va='top')\n",
    "    plt.text(0.05, 0.8, short_desc, fontsize=10, va='top', wrap=True)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Let's also visualize the distribution of adoption speeds and other features\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Subplot 1: Adoption speed distribution\n",
    "plt.subplot(2, 3, 1)\n",
    "adoption_counts = train_data[label_col].value_counts().sort_index()\n",
    "ax = sns.barplot(x=adoption_counts.index, y=adoption_counts.values)\n",
    "plt.title('Adoption Speed Distribution')\n",
    "plt.xlabel('Adoption Speed (0=Slow, 1=Fast)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add actual count values on top of bars\n",
    "for i, count in enumerate(adoption_counts.values):\n",
    "    ax.text(i, count + 5, str(count), ha='center')\n",
    "\n",
    "# Subplot 2: Pet type distribution\n",
    "plt.subplot(2, 3, 2)\n",
    "type_counts = train_data['Type'].map({1: 'Dog', 2: 'Cat'}).value_counts()\n",
    "ax = sns.barplot(x=type_counts.index, y=type_counts.values)\n",
    "plt.title('Pet Type Distribution')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add actual count values on top of bars\n",
    "for i, count in enumerate(type_counts.values):\n",
    "    ax.text(i, count + 5, str(count), ha='center')\n",
    "\n",
    "# Subplot 3: Age distribution\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.histplot(train_data['Age'], bins=15, kde=True)\n",
    "plt.title('Pet Age Distribution (months)')\n",
    "plt.xlabel('Age')\n",
    "\n",
    "# Subplot 4: Gender distribution\n",
    "plt.subplot(2, 3, 4)\n",
    "gender_counts = train_data['Gender'].map({1: 'Male', 2: 'Female', 3: 'Mixed'}).value_counts()\n",
    "ax = sns.barplot(x=gender_counts.index, y=gender_counts.values)\n",
    "plt.title('Gender Distribution')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add actual count values on top of bars\n",
    "for i, count in enumerate(gender_counts.values):\n",
    "    ax.text(i, count + 5, str(count), ha='center')\n",
    "\n",
    "# Subplot 5: Description length distribution\n",
    "plt.subplot(2, 3, 5)\n",
    "# Create a copy of the data for visualization only, don't modify the original data\n",
    "viz_data = train_data.copy()\n",
    "viz_data['desc_length'] = viz_data['Description'].apply(len)\n",
    "sns.histplot(viz_data['desc_length'], bins=15, kde=True)\n",
    "plt.title('Description Length Distribution')\n",
    "plt.xlabel('Character Count')\n",
    "\n",
    "# Subplot 6: Average adoption speed by pet type\n",
    "plt.subplot(2, 3, 6)\n",
    "pet_type_adoption = train_data.groupby('Type')[label_col].mean().reset_index()\n",
    "pet_type_adoption['Type'] = pet_type_adoption['Type'].map({1: 'Dog', 2: 'Cat'})\n",
    "sns.barplot(x='Type', y=label_col, data=pet_type_adoption)\n",
    "plt.title('Average Adoption Speed by Pet Type')\n",
    "plt.ylabel('Adoption Speed (Higher = Faster)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our dataset includes multiple types of data for each pet: an image, a text description, and various tabular features. The target variable is `AdoptionSpeed`, which indicates whether the pet was adopted quickly (1) or slowly (0).\n",
    "\n",
    "## 3. Training with MultiModalPredictor\n",
    "\n",
    "Now that our data is ready, let's train our multimodal model using AutoGluon's `MultiModalPredictor`. The `MultiModalPredictor` will automatically handle the different data types and train appropriate models for each modality, then combine them to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor\n",
    "\n",
    "# Initialize the MultiModalPredictor\n",
    "predictor = MultiModalPredictor(\n",
    "    label=label_col,\n",
    "    path='ag_petfinder_model',\n",
    "    presets='medium_quality'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's fit the predictor to our training data. We'll set a time limit to control the training duration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "predictor.fit(\n",
    "    train_data=train_data,\n",
    "    time_limit=300  # 5 minutes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding What Happens During Training\n",
    "\n",
    "During the training process, `MultiModalPredictor` performs several important steps:\n",
    "\n",
    "1. **Problem type detection**: Automatically detects whether it's a classification or regression problem\n",
    "2. **Modality detection**: Identifies which columns contain images, text, or tabular data\n",
    "3. **Model selection**: Selects appropriate models for each modality from the multimodal model pool\n",
    "4. **Feature extraction**: Processes each modality to extract meaningful features\n",
    "   - For images: Uses pre-trained vision models like ResNet or ViT\n",
    "   - For text: Uses NLP models like BERT or RoBERTa\n",
    "   - For tabular: Uses numeric and categorical feature processors\n",
    "5. **Fusion**: Combines features from different modalities using a fusion model (MLP or transformer)\n",
    "6. **Training**: Trains the end-to-end model to predict the target variable\n",
    "\n",
    "All of these steps happen automatically, allowing you to focus on your problem rather than the technical details of multimodal model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Making Predictions\n",
    "\n",
    "Now that we've trained our model, let's use it to make predictions on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "# We need to drop the label column when making predictions\n",
    "test_data_pred = test_data.drop(columns=[label_col])\n",
    "\n",
    "# Make predictions\n",
    "predictions = predictor.predict(test_data_pred)\n",
    "\n",
    "# Display the first few predictions\n",
    "print(\"Predicted adoption speeds (0=slow, 1=fast):\")\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification tasks, we can also get the prediction probabilities for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities\n",
    "# Use the same test data without the label column\n",
    "probs = predictor.predict_proba(test_data_pred)\n",
    "\n",
    "# Display the first few prediction probabilities\n",
    "print(\"Prediction probabilities (probability of fast adoption):\")\n",
    "print(probs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "Let's evaluate our model's performance on the test dataset using various metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "# Note: For evaluation, we use the full test data with labels\n",
    "evaluation_results = predictor.evaluate(test_data, metrics=['accuracy', 'f1', 'roc_auc'])\n",
    "\n",
    "# Display the evaluation results\n",
    "print(\"Evaluation Results:\")\n",
    "for metric, value in evaluation_results.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a few visualizations to better understand our model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Create confusion matrix\n",
    "y_true = test_data[label_col].values\n",
    "y_pred = predictions.values\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Slow', 'Fast'], \n",
    "            yticklabels=['Slow', 'Fast'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "# For binary classification, we need to use only the probabilities of the positive class (class 1)\n",
    "# If probs is a DataFrame with one column per class, select just the positive class column\n",
    "if isinstance(probs, pd.DataFrame) and probs.shape[1] > 1:\n",
    "    # Get the probability of the positive class (class 1)\n",
    "    # Assuming class 1 is the second column (index 1)\n",
    "    probs_positive_class = probs.iloc[:, 1]\n",
    "else:\n",
    "    # If probs is already a Series or 1D array\n",
    "    probs_positive_class = probs\n",
    "\n",
    "# Convert to numpy array\n",
    "probs_values = probs_positive_class.values\n",
    "\n",
    "# Now we use the 1D array of positive class probabilities\n",
    "fpr, tpr, _ = roc_curve(y_true, probs_values)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding Model Behavior\n",
    "\n",
    "To better understand how our model makes decisions, let's examine a few test examples and their predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random examples\n",
    "sample_indices = random.sample(range(len(test_data)), 5)\n",
    "samples = test_data.iloc[sample_indices].copy()\n",
    "sample_predictions = predictions.iloc[sample_indices]\n",
    "sample_probs = probs.iloc[sample_indices]\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    sample = test_data.iloc[idx]\n",
    "    true_label = sample[label_col]\n",
    "    pred_label = sample_predictions.iloc[i]\n",
    "    \n",
    "    # Handle probability correctly based on its type\n",
    "    prob = sample_probs.iloc[i]\n",
    "    if isinstance(prob, pd.Series) and len(prob) > 0:\n",
    "        # If it's a Series with multiple values, get the probability for the positive class (1)\n",
    "        if len(prob) > 1:\n",
    "            prob_value = prob.iloc[1]  # Probability of class 1\n",
    "        else:\n",
    "            prob_value = prob.iloc[0]  # If there's only one value\n",
    "    else:\n",
    "        # If it's already a scalar\n",
    "        prob_value = prob\n",
    "    \n",
    "    print(f\"\\nExample {i+1} (Pet #{idx}):\")\n",
    "    print(f\"True adoption speed: {true_label} ({'Fast' if true_label == 1 else 'Slow'})\")\n",
    "    print(f\"Predicted adoption speed: {pred_label} ({'Fast' if pred_label == 1 else 'Slow'})\")\n",
    "    print(f\"Probability of fast adoption: {prob_value:.4f}\")\n",
    "    \n",
    "    # Display the image\n",
    "    img_path = sample[image_col]\n",
    "    display(Image(filename=img_path))\n",
    "    \n",
    "    # Display key features\n",
    "    print(\"Key features:\")\n",
    "    for feature in ['Type', 'Age', 'Breed1', 'Gender', 'Color1', 'MaturitySize']:\n",
    "        print(f\"- {feature}: {sample[feature]}\")\n",
    "    \n",
    "    # Display a short version of the description\n",
    "    desc = sample['Description']\n",
    "    short_desc = desc[:200] + '...' if len(desc) > 200 else desc\n",
    "    print(f\"Description excerpt: {short_desc}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Features: Handling Missing Modalities\n",
    "\n",
    "In real-world applications, data is often incomplete. A key advantage of multimodal learning is the ability to handle missing modalities. Let's see how our model performs when certain modalities are missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data with missing image modality\n",
    "test_data_no_images = test_data.copy()\n",
    "test_data_no_images[image_col] = None  # Set all image paths to None\n",
    "\n",
    "# Create test data with missing text modality\n",
    "test_data_no_text = test_data.copy()\n",
    "test_data_no_text['Description'] = None  # Set all descriptions to None\n",
    "\n",
    "# Make predictions with missing modalities (without label column)\n",
    "predictions_no_images = predictor.predict(test_data_no_images.drop(columns=[label_col]))\n",
    "predictions_no_text = predictor.predict(test_data_no_text.drop(columns=[label_col]))\n",
    "\n",
    "# Compare evaluation metrics with different modalities\n",
    "print(\"Evaluation with all modalities:\")\n",
    "eval_full = predictor.evaluate(test_data)\n",
    "print(eval_full)\n",
    "\n",
    "print(\"\\nEvaluation without images:\")\n",
    "eval_no_images = predictor.evaluate(test_data_no_images)\n",
    "print(eval_no_images)\n",
    "\n",
    "print(\"\\nEvaluation without text:\")\n",
    "eval_no_text = predictor.evaluate(test_data_no_text)\n",
    "print(eval_no_text)\n",
    "\n",
    "# Create a summary table with available metrics\n",
    "# First, identify common metrics across all evaluations\n",
    "common_metrics = []\n",
    "for metric in eval_full.keys():\n",
    "    if metric in eval_no_images and metric in eval_no_text:\n",
    "        common_metrics.append(metric)\n",
    "\n",
    "if common_metrics:\n",
    "    print(\"\\nComparison of common metrics:\")\n",
    "    results_data = {}\n",
    "    \n",
    "    for metric in common_metrics:\n",
    "        results_data[metric] = [\n",
    "            eval_full[metric],\n",
    "            eval_no_images[metric],\n",
    "            eval_no_text[metric]\n",
    "        ]\n",
    "    \n",
    "    # Create DataFrame with rows as metrics and columns as modality configurations\n",
    "    results = pd.DataFrame(\n",
    "        results_data,\n",
    "        index=['All Modalities', 'No Images', 'No Text']\n",
    "    ).T  # Transpose to have metrics as rows\n",
    "    \n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Missing Modalities\n",
    "\n",
    "The results above show how the model's performance changes when certain modalities are missing. This can help us understand:\n",
    "\n",
    "1. The relative importance of each modality for the prediction task\n",
    "2. The model's robustness to missing data\n",
    "3. The complementary nature of different modalities\n",
    "\n",
    "In many real-world applications, having a model that can gracefully handle missing modalities is crucial for deployment in production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Customization: Training with Different Configurations\n",
    "\n",
    "AutoGluon's `MultiModalPredictor` supports a variety of configurations to customize the training process. Let's explore some of these options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom hyperparameter configuration\n",
    "hyperparameters = {\n",
    "    # Use a specific text backbone\n",
    "    'model.hf_text.checkpoint_name': 'google/electra-small-discriminator',\n",
    "    \n",
    "    # Use DINOv3 image backbone\n",
    "    'model.timm_image.checkpoint_name': 'timm/vit_small_patch14_dinov2.lvd142m',\n",
    "    \n",
    "    # Customize training parameters\n",
    "    'optim.lr': 5e-5,\n",
    "    \"optim.max_epochs\": 30,\n",
    "    'env.batch_size':64,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model with custom hyperparameters\n",
    "custom_predictor = MultiModalPredictor(\n",
    "    label=label_col,\n",
    "    path='ag_petfinder_custom_model'\n",
    ")\n",
    "\n",
    "custom_predictor.fit(\n",
    "    train_data=train_data,\n",
    "    hyperparameters=hyperparameters,\n",
    "    time_limit=600  # 10 minutes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the custom model\n",
    "custom_eval = custom_predictor.evaluate(test_data, metrics=['accuracy', 'f1'])\n",
    "print(\"Custom model evaluation:\")\n",
    "print(custom_eval)\n",
    "\n",
    "# Compare with the default model\n",
    "default_eval = predictor.evaluate(test_data, metrics=['accuracy', 'f1'])\n",
    "print(\"\\nDefault model evaluation:\")\n",
    "print(default_eval)\n",
    "\n",
    "# Create comparison table for common metrics\n",
    "common_metrics = []\n",
    "for metric in default_eval.keys():\n",
    "    if metric in custom_eval:\n",
    "        common_metrics.append(metric)\n",
    "\n",
    "if common_metrics:\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    comparison_data = {}\n",
    "    for metric in common_metrics:\n",
    "        comparison_data[metric] = [default_eval[metric], custom_eval[metric]]\n",
    "    \n",
    "    comparison = pd.DataFrame(\n",
    "        comparison_data,\n",
    "        index=['Default Model', 'Custom Model']\n",
    "    ).T  # Transpose to have metrics as rows\n",
    "    \n",
    "    print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Working with Text-Only Data\n",
    "\n",
    "While the PetFinder dataset contains multiple modalities, AutoGluon's `MultiModalPredictor` can also work effectively with single-modality data. Let's see how to use it for a text-only classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification Example\n",
    "\n",
    "Let's create a simple text classification task using just the pet descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text-only datasets\n",
    "text_train_data = train_data[['Description', label_col]].copy()\n",
    "text_test_data = test_data[['Description', label_col]].copy()\n",
    "\n",
    "# Train a text-only model\n",
    "text_predictor = MultiModalPredictor(\n",
    "    label=label_col,\n",
    "    path='ag_petfinder_text_model',\n",
    "    presets='medium_quality'\n",
    ")\n",
    "\n",
    "text_predictor.fit(\n",
    "    train_data=text_train_data,\n",
    "    time_limit=180  # 3 minutes\n",
    ")\n",
    "\n",
    "# Evaluate the text-only model\n",
    "# Note: For evaluation, keep the label column\n",
    "text_eval = text_predictor.evaluate(text_test_data, metrics=['accuracy', 'f1'])\n",
    "print(\"Text-only model performance:\")\n",
    "for metric, value in text_eval.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison: Text-Only vs. Full Multimodal\n",
    "\n",
    "Let's compare the performance of our text-only model with the full multimodal model to understand the value of including images and tabular features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print individual evaluations\n",
    "print(\"Multimodal model evaluation:\")\n",
    "print(default_eval)\n",
    "\n",
    "print(\"\\nText-only model evaluation:\")\n",
    "print(text_eval)\n",
    "\n",
    "# Create comparison table for common metrics\n",
    "common_metrics = []\n",
    "for metric in default_eval.keys():\n",
    "    if metric in text_eval:\n",
    "        common_metrics.append(metric)\n",
    "\n",
    "if common_metrics:\n",
    "    print(\"\\nModality Comparison:\")\n",
    "    comparison_data = {}\n",
    "    for metric in common_metrics:\n",
    "        comparison_data[metric] = [\n",
    "            default_eval[metric],\n",
    "            text_eval[metric]\n",
    "        ]\n",
    "    \n",
    "    comparison_full = pd.DataFrame(\n",
    "        comparison_data,\n",
    "        index=['Multimodal', 'Text Only']\n",
    "    ).T  # Transpose to have metrics as rows\n",
    "    \n",
    "    print(comparison_full)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    comparison_full.plot(kind='bar', figsize=(8, 5))\n",
    "    plt.title('Performance Comparison: Multimodal vs. Text-Only Models')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Metric')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend(title='Model Type')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights from Modality Comparison\n",
    "\n",
    "From this comparison, we can observe how much the text descriptions alone contribute to predicting adoption speed versus the full multimodal approach that incorporates images and tabular features. This analysis helps us understand the relative importance of different modalities in our specific prediction task.\n",
    "\n",
    "The performance difference demonstrates the value of combining multiple data types when available, but also shows that text-only models can still provide reasonable results when images or other modalities are unavailable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Saving and Loading\n",
    "\n",
    "Once you've trained a model, you might want to save it for future use or load it in a different session. Here's how to do that with the `MultiModalPredictor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is automatically saved during training at the specified path\n",
    "print(f\"Model saved at: {predictor.path}\")\n",
    "\n",
    "# To load a saved model\n",
    "loaded_predictor = MultiModalPredictor.load(predictor.path)\n",
    "\n",
    "# Prepare a small subset of data for testing the loaded model (without label column)\n",
    "test_subset = test_data.iloc[:5].drop(columns=[label_col])\n",
    "\n",
    "# Verify that the loaded model works\n",
    "loaded_predictions = loaded_predictor.predict(test_subset)\n",
    "print(\"Predictions from loaded model:\")\n",
    "print(loaded_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Extracting Embeddings for Other Applications\n",
    "\n",
    "`MultiModalPredictor` can also be used to extract embeddings (feature vectors) from the data, which can be useful for other applications such as clustering, visualization, or as features for other models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings from test data (without label column)\n",
    "sample_data = test_data.iloc[:]\n",
    "embeddings = predictor.extract_embedding(sample_data.drop(columns=[label_col]))\n",
    "\n",
    "# Display the shape of the embeddings\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "\n",
    "# Visualize the first two dimensions of the embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(\n",
    "    embeddings[:, 0], \n",
    "    embeddings[:, 1],\n",
    "    c=sample_data[label_col],  # We can still use labels for coloring\n",
    "    cmap='viridis',\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.colorbar(label='Adoption Speed')\n",
    "plt.xlabel('Embedding Dimension 1')\n",
    "plt.ylabel('Embedding Dimension 2')\n",
    "plt.title('2D Visualization of Multimodal Embeddings')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "### Workshop Recap\n",
    "\n",
    "In this workshop, we explored AutoGluon's `MultiModalPredictor` for handling text, image, and tabular data simultaneously. We trained models to predict pet adoption speed using a combination of pet images, text descriptions, and structured features. The workshop demonstrated how AutoGluon automatically handles different data types, simplifying the typically complex process of multimodal machine learning.\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "Our experiments confirmed that multimodal models often outperform single-modality models by leveraging complementary information. We showed this by comparing performance across models trained on all modalities versus those limited to text-only or image-only data. We also demonstrated the model's robustness when certain modalities are missing.\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "For effective multimodal learning with AutoGluon:\n",
    "- Ensure image columns contain valid file paths and text data is reasonably clean\n",
    "- Drop label columns when making predictions but include them for evaluation\n",
    "- Consider longer training times for better performance\n",
    "- Save models for future use with the `.load()` method\n",
    "- Extract embeddings when you need representations for downstream tasks\n",
    "\n",
    "### Additional Capabilities\n",
    "\n",
    "Beyond what we've covered, AutoGluon MultiModal supports:\n",
    "\n",
    "- **Computer Vision**: Object detection, semantic segmentation, and instance segmentation\n",
    "- **Natural Language Processing**: Named entity recognition, text classification, and question answering\n",
    "- **Cross-Modal Tasks**: Text-to-image matching and multimodal retrieval\n",
    "- **Advanced Techniques**: Zero-shot learning, few-shot learning, multi-task learning, and model distillation\n",
    "\n",
    "### Further Learning\n",
    "\n",
    "For specialized applications, consult the [AutoGluon documentation](https://auto.gluon.ai/stable/tutorials/multimodal/index.html), which provides comprehensive guides for adapting models to specific domains, optimizing for production deployment, and configuring advanced model architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Next Steps\n",
    "\n",
    "Please continue to the next workshop section **\"Train and deploy AutoGluon predictors on SageMaker\"** on Workshop Studio."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
